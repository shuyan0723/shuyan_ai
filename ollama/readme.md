# ollama

它是一个让你能通过简单命令在本地轻松下载，运行和管理大语言模型的工具，支持
GPU加速和类OPENAI 接口，适合本地部署开发。

Meta Llama 羊驼
deepseek-r1:1.5b   参数的尺寸
Qwen    

在11434端口提供api 调用
http://localhost:11434/api/chat